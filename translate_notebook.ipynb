{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook contains code for translating sentence-pairs to create initial datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjfzxXZLHed_",
        "outputId": "5d111dda-c9e7-4ba0-ff1e-bf885996d443"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
        "%cd indicTrans\n",
        "# clone requirements repositories\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone https://github.com/rsennrich/subword-nmt.git\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeYW2BJhlJvx",
        "outputId": "6113cc6c-fd32-4752-a9ef-748ba24f6303"
      },
      "outputs": [],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
        "!pip install mosestokenizer subword-nmt\n",
        "# Install fairseq from source\n",
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "%cd fairseq\n",
        "# !git checkout da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d\n",
        "!pip install ./\n",
        "!pip install xformers\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data\t      extracting_data.py   requirements.txt   train.tsv\n",
            "dev.tsv       fairseq\t\t   sentences_in.txt   translate_notebook.ipynb\n",
            "en2indic.zip  indic_nlp_resources  sentences_out.txt\n",
            "en-indic      indicTrans\t   test.tsv\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TktUu9NW_PLq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Blocksparse is not available: the current GPU does not expose Tensor cores\n"
          ]
        }
      ],
      "source": [
        "# add fairseq folder to python path\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \"fairseq/\"\n",
        "# sanity check to see if fairseq is installed\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_4JxNdRlPQB",
        "outputId": "7f3bee08-d634-49e7-cf99-339a5457f185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/neham/vibhhu/project_final/indicTrans\n"
          ]
        }
      ],
      "source": [
        "# # download the indictrans model\n",
        "\n",
        "# downloading the en-indic model\n",
        "# !wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/en2indic.zip\n",
        "# !unzip en2indic.zip\n",
        "\n",
        "# # # downloading the indic-indic model\n",
        "# # !wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.zip\n",
        "# # !unzip m2m.zip\n",
        "\n",
        "%cd indicTrans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTnWbHqY01-B",
        "outputId": "cb9676ca-8a08-456f-9cbb-0f0b5edec370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing vocab and bpe\n",
            "Initializing model for translation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-02 21:21:07 | INFO | fairseq.tasks.translation | [SRC] dictionary: 32104 types\n",
            "2023-05-02 21:21:07 | INFO | fairseq.tasks.translation | [TGT] dictionary: 35888 types\n"
          ]
        }
      ],
      "source": [
        "from indicTrans.inference.engine import Model\n",
        "\n",
        "en2indic_model = Model(expdir='../en-indic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fQkqGsWuren",
        "outputId": "0562f09b-6510-4096-b7a1-07486fa12ac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI4B_Demo\t\t\t\tinterface\n",
            "api\t\t\t\t\tjoint_score.sh\n",
            "apply_bpe_traindevtest_notag.sh\t\tjoint_translate.sh\n",
            "apply_single_bpe_traindevtest_notag.sh\tlearn_bpe.sh\n",
            "binarize_training_exp.sh\t\tlearn_single_bpe.sh\n",
            "compute_bleu.sh\t\t\t\tlegacy\n",
            "indic_nlp_library\t\t\tLICENSE\n",
            "indic_nlp_resources\t\t\tmodel_configs\n",
            "indictrans_fairseq_inference.ipynb\tprepare_data_joint_training.sh\n",
            "indicTrans_Finetuning.ipynb\t\tprepare_data.sh\n",
            "indicTrans_hosted_api_inference.ipynb\tREADME.md\n",
            "indicTrans_python_interface.ipynb\tsample_images\n",
            "IndicTrans_training.ipynb\t\tscripts\n",
            "inference\t\t\t\tsubword-nmt\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gXPZUmMfr_0D"
      },
      "outputs": [],
      "source": [
        "en_sents=[]\n",
        "with open('../nonparaphrases_out.txt') as f:\n",
        "    en_sents = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTp2NOgQ__sB",
        "outputId": "7d944657-d256-430d-c206-df7963817052"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4000/4000 [00:01<00:00, 3981.69it/s]\n",
            "2023-05-02 21:21:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-05-02 21:21:22 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2023-05-02 21:21:22 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2023-05-02 21:21:22 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n"
          ]
        }
      ],
      "source": [
        "a=en2indic_model.batch_translate(en_sents[:4000], 'en', 'hi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3447/3447 [23:59<00:00,  2.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
        "model.to(0)\n",
        "\n",
        "# Example English sentences\n",
        "english_sentences = en_sents\n",
        "\n",
        "# Tokenize the English sentences\n",
        "inputs = tokenizer(english_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "inputs.to(0)\n",
        "# Set the batch size\n",
        "batch_size = 8\n",
        "\n",
        "# Generate the Hindi translations in batches\n",
        "hindi_sentences = []\n",
        "for i in tqdm(range(0, len(inputs.input_ids), batch_size)):\n",
        "    input_ids_batch = inputs.input_ids[i:i+batch_size]\n",
        "    attention_mask_batch = inputs.attention_mask[i:i+batch_size]\n",
        "    outputs = model.generate(input_ids_batch, attention_mask=attention_mask_batch, \n",
        "                             max_length=40, num_beams=4, early_stopping=True)\n",
        "    for output in outputs:\n",
        "        hindi_sentence = tokenizer.decode(output, skip_special_tokens=True)\n",
        "        hindi_sentences.append(hindi_sentence)\n",
        "\n",
        "print(\"Done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'बीबाजी ने अपने अलग विचारों को स्वाग काल की किताब के माध्यम से फैला दिया, कजेल कोआ, आदि.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = open('hindi_output_nonpara.txt','w')\n",
        "for item in a:\n",
        "\tfile.write(item+\"\\n\")\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "VFXrCNZGEN7Z",
        "outputId": "143d20ae-92f5-4a8d-c9ed-d638a5bb3977"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 4304.06it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/usr/local/lib/python3.7/dist-packages/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The pandemic has caused global social and economic disruption. This has led to the worlds largest recession since the Great Depression. This led to the postponement or cancellation of sporting, religious, political and cultural events. Due to this fear, there was a shortage of supply as most of the people purchased the items like masks, sanitizers etc.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# ta_paragraph = \"\"\"இத்தொற்றுநோய் உலகளாவிய சமூக மற்றும் பொருளாதார சீர்குலைவை ஏற்படுத்தியுள்ளது.இதனால் பெரும் பொருளாதார மந்தநிலைக்குப் பின்னர் உலகளவில் மிகப்பெரிய மந்தநிலை ஏற்பட்டுள்ளது. இது விளையாட்டு,மத, அரசியல் மற்றும் கலாச்சார நிகழ்வுகளை ஒத்திவைக்க அல்லது ரத்து செய்ய வழிவகுத்தது.\n",
        "# அச்சம் காரணமாக முகக்கவசம், கிருமிநாசினி உள்ளிட்ட பொருட்களை அதிக நபர்கள் வாங்கியதால் விநியோகப் பற்றாக்குறை ஏற்பட்டது.\"\"\"\n",
        "\n",
        "# indic2en_model.translate_paragraph(ta_paragraph, 'ta', 'en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi_D7s_VIjis"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
